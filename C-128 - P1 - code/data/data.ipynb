{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45cd2dee-5360-4a68-8838-0f5b2b69b674",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "166653eb-bbe4-4b64-a408-4800deefbb02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_csv(\n",
    "    out_path: str = \"data/sample_data.csv\",\n",
    "    n: int = 1_000_000,\n",
    "    groups: int = 50,\n",
    "    seed: int = 42,\n",
    "    chunk_size: int = 100_000,\n",
    "    start_ts: str = \"2020-01-01T00:00:00\",\n",
    "    freq_seconds: int = 1,\n",
    "    user_id_max_multiplier: int = 2,\n",
    "    compress: bool = False\n",
    "):\n",
    "    rng = np.random.default_rng(seed)\n",
    "\n",
    "    # ensure output folder exists\n",
    "    os.makedirs(os.path.dirname(out_path) or \".\", exist_ok=True)\n",
    "\n",
    "    compression = \"gzip\" if compress else None\n",
    "    header_written = False\n",
    "\n",
    "    # We'll generate timestamps deterministically as start + i*freq_seconds\n",
    "    start = pd.to_datetime(start_ts)\n",
    "\n",
    "    rows_remaining = n\n",
    "    offset = 0\n",
    "    batch_idx = 0\n",
    "\n",
    "    print(f\"Generating {n} rows to '{out_path}' (groups={groups}, chunk={chunk_size}, compress={compress})\")\n",
    "\n",
    "    while rows_remaining > 0:\n",
    "        cur_chunk = min(chunk_size, rows_remaining)\n",
    "\n",
    "        # generate arrays\n",
    "        group = rng.integers(0, groups, size=cur_chunk, dtype=np.int32)\n",
    "        # value distribution: base normal plus small group-dependent shift\n",
    "        value = rng.normal(loc=100.0, scale=20.0, size=cur_chunk).astype(np.float32) + (group * 0.5).astype(np.float32)\n",
    "        user_id = rng.integers(0, n * user_id_max_multiplier, size=cur_chunk, dtype=np.int32)\n",
    "\n",
    "        # timestamps for this chunk\n",
    "        # compute seconds offsets for chunk\n",
    "        start_offset_seconds = offset * freq_seconds\n",
    "        secs = np.arange(start_offset_seconds, start_offset_seconds + cur_chunk * freq_seconds, freq_seconds, dtype=np.int64)\n",
    "        ts = (start + pd.to_timedelta(secs, unit=\"s\")).astype(\"datetime64[ns]\")\n",
    "\n",
    "        df_chunk = pd.DataFrame({\n",
    "            \"user_id\": user_id,\n",
    "            \"group\": group,\n",
    "            \"value\": value,\n",
    "            \"ts\": ts\n",
    "        })\n",
    "\n",
    "        # write chunk to CSV (append after first chunk)\n",
    "        if not header_written:\n",
    "            df_chunk.to_csv(out_path, index=False, compression=compression)\n",
    "            header_written = True\n",
    "        else:\n",
    "            # append without writing header\n",
    "            df_chunk.to_csv(out_path, index=False, header=False, mode=\"a\", compression=compression)\n",
    "\n",
    "        offset += cur_chunk\n",
    "        rows_remaining -= cur_chunk\n",
    "        batch_idx += 1\n",
    "\n",
    "        print(f\"  wrote chunk {batch_idx} ({cur_chunk} rows), {rows_remaining} rows remaining\")\n",
    "\n",
    "    print(f\"Done. Wrote {n} rows to {out_path} (compression={compression})\")\n",
    "\n",
    "def parse_args():\n",
    "    p = argparse.ArgumentParser(description=\"Generate synthetic dataset for approx-query-engine\")\n",
    "    p.add_argument(\"--out\", default=\"data/sample_data.csv\", help=\"Output CSV path (use .gz if compress)\")\n",
    "    p.add_argument(\"--n\", type=int, default=1_000_000, help=\"Total number of rows to generate\")\n",
    "    p.add_argument(\"--groups\", type=int, default=50, help=\"Number of distinct groups\")\n",
    "    p.add_argument(\"--seed\", type=int, default=42, help=\"Random seed\")\n",
    "    p.add_argument(\"--chunk\", type=int, default=100_000, help=\"Chunk size for writing (rows per write)\")\n",
    "    p.add_argument(\"--start-ts\", default=\"2020-01-01T00:00:00\", help=\"Start timestamp (ISO format)\")\n",
    "    p.add_argument(\"--freq-seconds\", type=int, default=1, help=\"Timestamp frequency in seconds\")\n",
    "    p.add_argument(\"--user-id-mult\", type=int, default=2, help=\"Multiplier for user_id max (n * mult)\")\n",
    "    p.add_argument(\"--compress\", action=\"store_true\", help=\"Write gzip compressed CSV\")\n",
    "    return p.parse_args()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    args = parse_args()\n",
    "    generate_csv(\n",
    "        out_path=args.out,\n",
    "        n=args.n,\n",
    "        groups=args.groups,\n",
    "        seed=args.seed,\n",
    "        chunk_size=args.chunk,\n",
    "        start_ts=args.start_ts,\n",
    "        freq_seconds=args.freq_seconds,\n",
    "        user_id_max_multiplier=args.user_id_mult,\n",
    "        compress=args.compress\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
